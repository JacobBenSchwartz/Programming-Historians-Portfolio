{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c766163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea41a98",
   "metadata": {},
   "source": [
    "# Analyzing Multilingual French and Russian Test using NLTK, spaCy, and Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596004a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in /home/codespace/.python/current/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/codespace/.python/current/lib/python3.12/site-packages (from NLTK) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from NLTK) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from NLTK) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from NLTK) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f97a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spaCy in /home/codespace/.python/current/lib/python3.12/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spaCy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spaCy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spaCy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spaCy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spaCy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from spaCy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spaCy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spaCy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /home/codespace/.python/current/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47beb9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from Stanza)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from Stanza) (2.2.4)\n",
      "Collecting protobuf>=3.15.0 (from Stanza)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from Stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from Stanza) (3.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from Stanza) (2.6.0+cpu)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from Stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->Stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.3.0->Stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->Stanza) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->Stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->Stanza) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->Stanza) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.3.0->Stanza) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, emoji, Stanza\n",
      "Successfully installed Stanza-1.10.1 emoji-2.14.1 protobuf-6.30.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e193f4",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f93a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "— Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\n",
      "\n",
      "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:\n",
      "\n",
      "«Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».\n",
      "\n",
      "— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
      "\n",
      "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
      "\n",
      "— Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "war_and_peace = \"\"\"\n",
    "— Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\n",
    "\n",
    "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:\n",
    "\n",
    "«Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».\n",
    "\n",
    "— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
    "\n",
    "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
    "\n",
    "— Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n",
    "\"\"\"\n",
    "print(war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff48518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — Eh bien, mon prince. Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.  Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер. Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими). В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures. Annette Scherer».  — Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.  Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку. Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.  — Avant tout dites moi, comment vous allez, chère amie? Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка. \n"
     ]
    }
   ],
   "source": [
    "cleaned_war_and_peace = war_and_peace.replace(\"\\n\",\" \")\n",
    "print(cleaned_war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f4f165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c46ea2",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f5b77",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef2fa292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk_sent_tokenized = sent_tokenize(cleaned_war_and_peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78db07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — Eh bien, mon prince.\n",
      "Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte.\n",
      "Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites.\n",
      "Ну, здравствуйте, здравствуйте.\n",
      "Je vois que je vous fais peur, садитесь и рассказывайте.\n",
      "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими).\n",
      "В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures.\n",
      "Annette Scherer».\n",
      "— Dieu, quelle virulente sortie!\n",
      "— отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.\n",
      "Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку.\n",
      "Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.\n",
      "— Avant tout dites moi, comment vous allez, chère amie?\n",
      "Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.\n"
     ]
    }
   ],
   "source": [
    "# Printing each sentence in our list\n",
    "for sent in nltk_sent_tokenized:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd603b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.\n"
     ]
    }
   ],
   "source": [
    "# printing the Russian sentence at index 5 in our list of sentences\n",
    "rus_sent = nltk_sent_tokenized[5]\n",
    "print('Russian: ' + rus_sent)\n",
    "\n",
    "# printing the French sentence at index 2\n",
    "fre_sent = nltk_sent_tokenized[13]\n",
    "print('French: ' + fre_sent)\n",
    "\n",
    "# printing the sentence in both French and Russian at index 4\n",
    "multi_sent = nltk_sent_tokenized[4]\n",
    "print('Multilang: ' + multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c9a45",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a743c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xx-sent-ud-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('xx_sent_ud_sm')\n"
     ]
    }
   ],
   "source": [
    "# downloading our multilingual sentence tokenizer\n",
    "!python3 -m spacy download xx_sent_ud_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81775b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ , — Eh bien, mon prince., Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte., Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites., Ну, здравствуйте, здравствуйте., Je vois que je vous fais peur, садитесь и рассказывайте.  , Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер., Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими)., В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  , «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures., Annette Scherer».  , — Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.  , Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку., Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.  , — Avant tout dites moi, comment vous allez, chère amie?, Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.]\n"
     ]
    }
   ],
   "source": [
    "# loading the multilingual sentence tokenizer we just downloaded\n",
    "nlp = spacy.load(\"xx_sent_ud_sm\")\n",
    "# applying the spaCy model to our text variable\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# assigning the tokenized sentences to a list so it's easier for us to manipulate them later\n",
    "spacy_sentences = list(doc.sents)\n",
    "\n",
    "# printing the sentences to our console\n",
    "print(spacy_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa9b1aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.  \n"
     ]
    }
   ],
   "source": [
    "# concatenating the Russian sentence and its language label\n",
    "spacy_rus_sent = str(spacy_sentences[6])\n",
    "print('Russian: ' + spacy_rus_sent)\n",
    "\n",
    "# concatenating the French sentence and its language label\n",
    "spacy_fre_sent = str(spacy_sentences[14])\n",
    "print('French: ' + spacy_fre_sent)\n",
    "\n",
    "# concatenating the French and Russian sentence and its label\n",
    "spacy_multi_sent = str(spacy_sentences[5])\n",
    "print('Multilang: ' + spacy_multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af364c56",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22ee09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:50:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 187MB/s]                     \n",
      "2025-05-08 21:50:42 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 21:50:43 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-05-08 21:50:43 INFO: Using device: cpu\n",
      "2025-05-08 21:50:43 INFO: Loading: langid\n",
      "2025-05-08 21:50:43 INFO: Done loading processors!\n",
      "2025-05-08 21:50:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 126MB/s]                     \n",
      "2025-05-08 21:50:43 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 21:50:43 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "=========================\n",
      "\n",
      "2025-05-08 21:50:43 INFO: Using device: cpu\n",
      "2025-05-08 21:50:43 INFO: Loading: tokenize\n",
      "2025-05-08 21:50:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['— Eh bien, mon prince.', 'Gênes et Lucques ne sont plus que des apanages, des поместья, de la famille Buonaparte.', 'Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites.', 'Ну, здравствуйте, здравствуйте.', 'Je vois que je vous fais peur, садитесь и рассказывайте.', 'Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.', 'Анна Павловна кашляла несколько дней, у нее был грипп, как она говорила (грипп был тогда новое слово, употреблявшееся только редкими).', 'В записочках, разосланных утром с красным лакеем, было написано без различия во всех:  «Si vous n’avez rien de mieux à faire, M. le comte (или mon prince), et si la perspective de passer la soirée chez une pauvre malade ne vous effraye pas trop, je serai charmée de vous voir chez moi entre 7 et 10 heures.', 'Annette Scherer».', '— Dieu, quelle virulente sortie! — отвечал, нисколько не смутясь такою встречей, вошедший князь, в придворном, шитом мундире, в чулках, башмаках, и звездах, с светлым выражением плоского лица.', 'Он говорил на том изысканном французском языке, на котором не только говорили, но и думали наши деды, и с теми тихими, покровительственными интонациями, которые свойственны состаревшемуcя в свете и при дворе значительному человеку.', 'Он подошел к Анне Павловне, поцеловал ее руку, подставив ей свою надушенную и сияющую лысину, и покойно уселся на диване.', '— Avant tout dites moi, comment vous allez, chère amie?', 'Успокойте меня, — сказал он, не изменяя голоса и тоном, в котором из-за приличия и участия просвечивало равнодушие и даже насмешка.']\n"
     ]
    }
   ],
   "source": [
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# setting up our tokenizer pipeline\n",
    "nlp = MultilingualPipeline(processors='tokenize')\n",
    "\n",
    "# applying the pipeline to our text\n",
    "doc = nlp(cleaned_war_and_peace)\n",
    "\n",
    "# printing all sentences to see how they tokenized\n",
    "print([sentence.text for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d52aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
      "French: — Avant tout dites moi, comment vous allez, chère amie?\n",
      "Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list to append our sentences to\n",
    "stanza_sentences = []\n",
    "\n",
    "# iterating through the sentence tokens created by the tokenizer pipeline and appending to the list\n",
    "for sentence in doc.sentences:\n",
    "  stanza_sentences.append(sentence.text)\n",
    "\n",
    "# printing our sentence that is only in Russian\n",
    "stanza_rus_sent = str(stanza_sentences[5])\n",
    "print('Russian: ' + stanza_rus_sent)\n",
    "\n",
    "# printing our sentence that is only in French\n",
    "stanza_fre_sent = str(stanza_sentences[12])\n",
    "print('French: ' + stanza_fre_sent)\n",
    "\n",
    "# printing our sentence in both French and Russian\n",
    "stanza_multi_sent = str(stanza_sentences[4])\n",
    "print('Multilang: ' + stanza_multi_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9594f",
   "metadata": {},
   "source": [
    "## Automatically Detecting Different Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfbac850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package crubadan to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package crubadan is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian estimate: rus\n",
      "French estimate: fra\n",
      "Multilingual estimate: rus\n"
     ]
    }
   ],
   "source": [
    "# downloading an NLTK corpus reader required by the TextCat module\n",
    "nltk.download('crubadan')\n",
    "\n",
    "# loading the TextCat package and applying it to each of our sentences\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "rus_estimate = tcat.guess_language(rus_sent)\n",
    "fre_estimate = tcat.guess_language(fre_sent)\n",
    "multi_estimate = tcat.guess_language(multi_sent)\n",
    "\n",
    "# printing the results\n",
    "print('Russian estimate: ' + rus_estimate)\n",
    "print('French estimate: ' + fre_estimate)\n",
    "print('Multilingual estimate: ' + multi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a35e923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy_langdetect in /home/codespace/.python/current/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: pytest in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy_langdetect) (8.3.5)\n",
      "Requirement already satisfied: langdetect==1.0.7 in /home/codespace/.python/current/lib/python3.12/site-packages (from spacy_langdetect) (1.0.7)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from langdetect==1.0.7->spacy_langdetect) (1.17.0)\n",
      "Requirement already satisfied: iniconfig in /home/codespace/.python/current/lib/python3.12/site-packages (from pytest->spacy_langdetect) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from pytest->spacy_langdetect) (24.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from pytest->spacy_langdetect) (1.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0062dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'ru', 'score': 0.9999980497958653}\n",
      "{'language': 'fr', 'score': 0.9999971238960873}\n",
      "{'language': 'bg', 'score': 0.714281887007078}\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "# Solution found here: https://stackoverflow.com/questions/66712753/how-to-use-languagedetector-from-spacy-langdetect-package\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "# Setting up our pipeline\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "# Running the language detection on each sentence and printing the results\n",
    "rus_doc = nlp(spacy_rus_sent)\n",
    "print(rus_doc._.language)\n",
    "\n",
    "fre_doc = nlp(spacy_fre_sent)\n",
    "print(fre_doc._.language)\n",
    "\n",
    "multi_doc = nlp(spacy_multi_sent)\n",
    "print(multi_doc._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e7174fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:53:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 98.4MB/s]                    \n",
      "2025-05-08 21:53:30 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 21:53:30 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-05-08 21:53:30 INFO: Using device: cpu\n",
      "2025-05-08 21:53:30 INFO: Loading: langid\n",
      "2025-05-08 21:53:30 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\tru\n",
      "— Avant tout dites moi, comment vous allez, chère amie?\tfr\n",
      "Je vois que je vous fais peur, садитесь и рассказывайте.\tfr\n"
     ]
    }
   ],
   "source": [
    "# importing our models required for language detection\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "\n",
    "# setting up our pipeline\n",
    "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\")\n",
    "\n",
    "# specifying which sentences to run the detection on, then running the detection code\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "docs = [Document([], text=text) for text in docs]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the text of each sentence alongside the language estimates\n",
    "print(\"\\n\".join(f\"{doc.text}\\t{doc.lang}\" for doc in docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fdb976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "tokenized_sent = wordpunct_tokenize(multi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f8ba2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the regex package so we can use a regular expression\n",
    "import regex\n",
    "# importing the string package to detect punctuation\n",
    "from string import punctuation\n",
    "\n",
    "# setting empty lists we will later populate with our words\n",
    "cyrillic_words = []\n",
    "latin_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "069437bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['садитесь', 'и', 'рассказывайте']\n",
      "['Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur']\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_sent:\n",
    "  if word in punctuation:\n",
    "    continue\n",
    "  else:\n",
    "    if regex.search(r'\\p{IsCyrillic}', word):\n",
    "      cyrillic_words.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b18c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyrillic estimate: rus\n",
      "Latin estimate: fra\n"
     ]
    }
   ],
   "source": [
    "# joining the lists into a string, with each word separated by a space (' ')\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# now we use TextCat again to detect their languages\n",
    "tcat = nltk.classify.textcat.TextCat()\n",
    "multi_estimate_1 = tcat.guess_language(cyrillic_only_list)\n",
    "multi_estimate_2 = tcat.guess_language(latin_only_list)\n",
    "\n",
    "# printing our estimates\n",
    "print('Cyrillic estimate: ' + multi_estimate_1)\n",
    "print('Latin estimate: ' + multi_estimate_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f5d89",
   "metadata": {},
   "source": [
    "## Part-of-Speech-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7443c",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9efb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from ru-core-news-sm==3.8.0) (2.0.3)\n",
      "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: setuptools>=68.2.2 in /home/codespace/.local/lib/python3.12/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (76.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n",
      "Так ADV\n",
      "говорила VERB\n",
      "в ADP\n",
      "июле NOUN\n",
      "1805 ADJ\n",
      "года NOUN\n",
      "известная ADJ\n",
      "Анна PROPN\n",
      "Павловна PROPN\n",
      "Шерер PROPN\n",
      ", PUNCT\n",
      "фрейлина NOUN\n",
      "и CCONJ\n",
      "приближенная ADJ\n",
      "императрицы NOUN\n",
      "Марии PROPN\n",
      "Феодоровны PROPN\n",
      ", PUNCT\n",
      "встречая VERB\n",
      "важного ADJ\n",
      "и CCONJ\n",
      "чиновного ADJ\n",
      "князя NOUN\n",
      "Василия PROPN\n",
      ", PUNCT\n",
      "первого ADJ\n",
      "приехавшего VERB\n",
      "на ADP\n",
      "ее DET\n",
      "вечер NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# downloading our Russian model from spaCy\n",
    "!python -m spacy download ru_core_news_sm\n",
    "\n",
    "\n",
    "# loading the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_rus_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01266889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "— PUNCT\n",
      "Avant ADP\n",
      "tout PRON\n",
      "dites VERB\n",
      "moi PRON\n",
      ", PUNCT\n",
      "comment ADV\n",
      "vous PRON\n",
      "allez VERB\n",
      ", PUNCT\n",
      "chère ADJ\n",
      "amie NOUN\n",
      "? PUNCT\n"
     ]
    }
   ],
   "source": [
    "# downloading our French model from spaCy\n",
    "!python -m spacy download fr_core_news_sm\n",
    "\n",
    "\n",
    "# loading the corpus\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# applying the model\n",
    "doc = nlp(spacy_fre_sent)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02cb75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['садитесь', 'и', 'рассказывайте']\n",
      "['Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur', 'Je', 'vois', 'que', 'je', 'vous', 'fais', 'peur']\n"
     ]
    }
   ],
   "source": [
    "# creating our blank lists to append to later\n",
    "cyrillic_words_punct = []\n",
    "latin_words_punct = []\n",
    "\n",
    "# initializing a blank string to keep track of the last list we appended to\n",
    "last_appended_list = ''\n",
    "\n",
    "# iterating through our words and appending based on whether a Cyrillic character was detected\n",
    "for word in tokenized_sent:\n",
    "  if regex.search(r'\\p{IsCyrillic}', word):\n",
    "    cyrillic_words_punct.append(word)\n",
    "    # updating our string to track the list we appended a word to\n",
    "    last_appended_list = 'cyr'\n",
    "  else:\n",
    "    # handling punctuation by appending it to our most recently used list\n",
    "    if word in punctuation:\n",
    "        if last_appended_list == 'cyr':\n",
    "            cyrillic_words_punct.append(word)\n",
    "        elif last_appended_list == 'lat':\n",
    "            latin_words_punct.append(word)\n",
    "    else:\n",
    "        latin_words.append(word)\n",
    "        last_appended_list = 'lat'\n",
    "\n",
    "print(cyrillic_words)\n",
    "print(latin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7071ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь и рассказывайте\n",
      "Je vois que je vous fais peur Je vois que je vous fais peur\n"
     ]
    }
   ],
   "source": [
    "# joining the lists to strings\n",
    "cyrillic_only_list = ' '.join(cyrillic_words)\n",
    "latin_only_list = ' '.join(latin_words)\n",
    "\n",
    "# using our regular expression to remove extra whitespace before the punctuation marks\n",
    "cyr_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', cyrillic_only_list)\n",
    "lat_no_extra_space = regex.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', latin_only_list)\n",
    "\n",
    "# checking the results of the regular expression above\n",
    "print(cyr_no_extra_space)\n",
    "print(lat_no_extra_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4301cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь VERB\n",
      "и CCONJ\n",
      "рассказывайте VERB\n",
      "Je PRON\n",
      "vois VERB\n",
      "que SCONJ\n",
      "je PRON\n",
      "vous PRON\n",
      "fais VERB\n",
      "peur NOUN\n",
      "Je PRON\n",
      "vois VERB\n",
      "que SCONJ\n",
      "je PRON\n",
      "vous PRON\n",
      "fais VERB\n",
      "peur NOUN\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the text of each word and its POS tag\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# and doing the same with our French sentence\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5071d",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecfa34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:57:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 170MB/s]                     \n",
      "2025-05-08 21:57:24 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 21:57:25 INFO: Loading these models for language: ru (Russian):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | syntagrus        |\n",
      "| pos       | syntagrus_charlm |\n",
      "================================\n",
      "\n",
      "2025-05-08 21:57:25 INFO: Using device: cpu\n",
      "2025-05-08 21:57:25 INFO: Loading: tokenize\n",
      "2025-05-08 21:57:25 INFO: Loading: pos\n",
      "2025-05-08 21:57:27 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Так\tupos: ADV\n",
      "word: говорила\tupos: VERB\n",
      "word: в\tupos: ADP\n",
      "word: июле\tupos: NOUN\n",
      "word: 1805\tupos: ADJ\n",
      "word: года\tupos: NOUN\n",
      "word: известная\tupos: ADJ\n",
      "word: Анна\tupos: PROPN\n",
      "word: Павловна\tupos: PROPN\n",
      "word: Шерер\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: фрейлина\tupos: NOUN\n",
      "word: и\tupos: CCONJ\n",
      "word: приближенная\tupos: ADJ\n",
      "word: императрицы\tupos: NOUN\n",
      "word: Марии\tupos: PROPN\n",
      "word: Феодоровны\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: встречая\tupos: VERB\n",
      "word: важного\tupos: ADJ\n",
      "word: и\tupos: CCONJ\n",
      "word: чиновного\tupos: ADJ\n",
      "word: князя\tupos: NOUN\n",
      "word: Василия\tupos: PROPN\n",
      "word: ,\tupos: PUNCT\n",
      "word: первого\tupos: ADJ\n",
      "word: приехавшего\tupos: VERB\n",
      "word: на\tupos: ADP\n",
      "word: ее\tupos: DET\n",
      "word: вечер\tupos: NOUN\n",
      "word: .\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as Russian ('ru')\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')\n",
    "doc = nlp(stanza_rus_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d575ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 21:57:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 139MB/s]                     \n",
      "2025-05-08 21:57:37 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 21:57:38 INFO: Loading these models for language: fr (French):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-05-08 21:57:38 INFO: Using device: cpu\n",
      "2025-05-08 21:57:38 INFO: Loading: tokenize\n",
      "2025-05-08 21:57:38 INFO: Loading: mwt\n",
      "2025-05-08 21:57:38 INFO: Loading: pos\n",
      "2025-05-08 21:57:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: —\tupos: PUNCT\n",
      "word: Avant\tupos: ADP\n",
      "word: tout\tupos: PRON\n",
      "word: dites\tupos: VERB\n",
      "word: moi\tupos: PRON\n",
      "word: ,\tupos: PUNCT\n",
      "word: comment\tupos: ADV\n",
      "word: vous\tupos: PRON\n",
      "word: allez\tupos: VERB\n",
      "word: ,\tupos: PUNCT\n",
      "word: chère\tupos: ADJ\n",
      "word: amie\tupos: NOUN\n",
      "word: ?\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# loading our pipeline and applying it to our sentence, specifying our language as French ('fr')\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos')\n",
    "doc = nlp(stanza_fre_sent)\n",
    "\n",
    "# printing our words and POS tags\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaee6761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.',\n",
       " '— Avant tout dites moi, comment vous allez, chère amie?',\n",
       " 'Je vois que je vous fais peur, садитесь и рассказывайте.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61b69bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports so we can use Stanza's MultilingualPipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76a6e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 22:05:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 198MB/s]                     \n",
      "2025-05-08 22:05:49 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 22:05:49 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-05-08 22:05:49 INFO: Using device: cpu\n",
      "2025-05-08 22:05:49 INFO: Loading: langid\n",
      "2025-05-08 22:05:49 INFO: Done loading processors!\n",
      "2025-05-08 22:05:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 132MB/s]                     \n",
      "2025-05-08 22:05:49 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 22:05:50 INFO: Loading these models for language: ru (Russian):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | syntagrus        |\n",
      "| pos       | syntagrus_charlm |\n",
      "================================\n",
      "\n",
      "2025-05-08 22:05:50 INFO: Using device: cpu\n",
      "2025-05-08 22:05:50 INFO: Loading: tokenize\n",
      "2025-05-08 22:05:50 INFO: Loading: pos\n",
      "2025-05-08 22:05:52 INFO: Done loading processors!\n",
      "2025-05-08 22:05:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 130MB/s]                     \n",
      "2025-05-08 22:05:52 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 22:05:52 WARNING: Language fr package default expects mwt, which has been added\n",
      "2025-05-08 22:05:52 INFO: Loading these models for language: fr (French):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-05-08 22:05:52 INFO: Using device: cpu\n",
      "2025-05-08 22:05:52 INFO: Loading: tokenize\n",
      "2025-05-08 22:05:52 INFO: Loading: mwt\n",
      "2025-05-08 22:05:52 INFO: Loading: pos\n",
      "2025-05-08 22:05:54 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: —\tupos: PUNCT\n",
      "word: Avant\tupos: ADP\n",
      "word: tout\tupos: PRON\n",
      "word: dites\tupos: VERB\n",
      "word: moi\tupos: PRON\n",
      "word: ,\tupos: PUNCT\n",
      "word: comment\tupos: ADV\n",
      "word: vous\tupos: PRON\n",
      "word: allez\tupos: VERB\n",
      "word: ,\tupos: PUNCT\n",
      "word: chère\tupos: ADJ\n",
      "word: amie\tupos: NOUN\n",
      "word: ?\tupos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# running the multilingual pipeline on our French, Russian, and multilingual sentences simultaneously\n",
    "nlp = MultilingualPipeline(processors='tokenize,pos')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlp(docs)\n",
    "\n",
    "# printing the results\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(*[f'word: {word.text}\\tupos: {word.upos}'], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499591e",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdba03a",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "566ab34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "садитесь садиться\n",
      "и и\n",
      "рассказывайте рассказывать\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "doc = nlp(cyr_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb732695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je je\n",
      "vois voir\n",
      "que que\n",
      "je je\n",
      "vous vous\n",
      "fais faire\n",
      "peur peur\n",
      "Je je\n",
      "vois voir\n",
      "que que\n",
      "je je\n",
      "vous vous\n",
      "fais faire\n",
      "peur peur\n"
     ]
    }
   ],
   "source": [
    "# loading and applying the model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(lat_no_extra_space)\n",
    "\n",
    "# printing the words and their lemmas\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6942c",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f38dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 22:10:27 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 198MB/s]                     \n",
      "2025-05-08 22:10:27 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 22:10:27 INFO: Loading these models for language: multilingual ():\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| langid    | ud      |\n",
      "=======================\n",
      "\n",
      "2025-05-08 22:10:27 INFO: Using device: cpu\n",
      "2025-05-08 22:10:27 INFO: Loading: langid\n",
      "2025-05-08 22:10:28 INFO: Done loading processors!\n",
      "2025-05-08 22:10:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 192MB/s]                     \n",
      "2025-05-08 22:10:28 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-ru/resolve/v1.10.0/models/lemma/syntagrus_nocharlm.pt: 100%|██████████| 13.5M/13.5M [00:00<00:00, 54.2MB/s]\n",
      "2025-05-08 22:10:28 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-05-08 22:10:28 INFO: Using device: cpu\n",
      "2025-05-08 22:10:28 INFO: Loading: tokenize\n",
      "2025-05-08 22:10:28 INFO: Loading: lemma\n",
      "2025-05-08 22:10:31 INFO: Done loading processors!\n",
      "2025-05-08 22:10:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 194MB/s]                     \n",
      "2025-05-08 22:10:31 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-08 22:10:31 WARNING: Language fr package default expects mwt, which has been added\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/lemma/combined_nocharlm.pt: 100%|██████████| 5.41M/5.41M [00:00<00:00, 30.4MB/s]\n",
      "2025-05-08 22:10:31 INFO: Loading these models for language: fr (French):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-05-08 22:10:31 INFO: Using device: cpu\n",
      "2025-05-08 22:10:31 INFO: Loading: tokenize\n",
      "2025-05-08 22:10:31 INFO: Loading: mwt\n",
      "2025-05-08 22:10:31 INFO: Loading: lemma\n",
      "2025-05-08 22:10:32 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['так', 'говорить', 'в', 'июль', '1805', 'год', 'известный', 'Анна', 'Павловна', 'Шерер', ',', 'фрейлин', 'и', 'приближенный', 'императрица', 'Мария', 'феодоровный', ',', 'встречать', 'важный', 'и', 'чиновный', 'князь', 'Василий', ',', 'первый', 'приехать', 'на', 'она', 'вечер', '.']\n",
      "['—', 'avant', 'tout', 'dire', 'moi', ',', 'comment', 'vous', 'aller', ',', 'cher', 'amie', '?']\n",
      "['moi', 'voir', 'que', 'moi', 'vous', 'faire', 'peur', ',', 'садитесь', 'и', 'рассказывайте', '.']\n"
     ]
    }
   ],
   "source": [
    "# imports so we can run the multilingual pipeline\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "from stanza.pipeline.multilingual import MultilingualPipeline\n",
    "\n",
    "# adding the 'lemma' processor to the pipeline and running it on our sentences\n",
    "nlp = MultilingualPipeline(processors='tokenize,lemma')\n",
    "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
    "nlped_docs = nlp(docs)\n",
    "\n",
    "# iterating through each sentence's words and printing the lemmas\n",
    "for doc in nlped_docs:\n",
    "  lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]\n",
    "  print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
